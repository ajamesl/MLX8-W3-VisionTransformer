{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9137816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14b6fae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. Load MSNIST dataset\n",
    "# -----------------------------\n",
    "\n",
    "# This will stream the data, you don't have to download the full file\n",
    "# mnist_train = load_dataset(\"ylecun/mnist\", split=\"train\")\n",
    "\n",
    "# mnist_test = load_dataset(\"ylecun/mnist\", split=\"test\")\n",
    "\n",
    "\n",
    "##### Look into the normalisation #####\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "mnist_train = datasets.MNIST(\"./data\", train=True, download=False, transform=transform)\n",
    "\n",
    "mnist_test = datasets.MNIST(\"./data\", train=False, download=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6bc1349b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "img, label = mnist_train[0]\n",
    "print(img.shape)    # e.g., torch.Size([1, 28, 28])\n",
    "print(type(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a00c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch(img, patch_size=7):\n",
    "    # img shape: (1, 28, 28)\n",
    "    patches = img.unfold(1, patch_size, patch_size).unfold(2, patch_size, patch_size)\n",
    "    # shape: (1, 4, 4, 7, 7)\n",
    "    patches = patches.contiguous().view(1, -1, patch_size, patch_size)\n",
    "    # shape: (1, 16, 7, 7)\n",
    "    return patches.squeeze(0)  # (16, 7, 7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "baa66336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "img, label = mnist_train[0]  # img: (1, 28, 28)\n",
    "patches = patch(img, patch_size=7)\n",
    "print(patches.shape)  # Should print: torch.Size([16, 7, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "703d37d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 16, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "all_patches = [patch(img) for img, _ in mnist_train]  # list of (16,7,7)\n",
    "all_patches = torch.stack(all_patches)  # (N, 16, 7, 7)\n",
    "all_labels = torch.tensor([label for _, label in mnist_train])  # shape: (60000,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7450918c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 16, 7, 7])\n",
      "torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "print(all_patches.shape)  # Should print: torch.Size([60000, 16, 7, 7])\n",
    "print(all_labels.shape)  # Should print: torch.Size([60000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b161c899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 16, 49])\n"
     ]
    }
   ],
   "source": [
    "flat_patches = all_patches.view(60000, 16, -1)  # shape: (60000, 16, 49)\n",
    "print(flat_patches.shape)  # Should print: torch.Size([60000, 16, 49])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663cb22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "patch_size = 7\n",
    "embed_dim = 64\n",
    "patch_dim = patch_size * patch_size\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"A simple linear projection of patches to embeddings.\"\"\"\n",
    "    def __init__(self, patch_dim, embed_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(patch_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, num_patches, patch_dim)\n",
    "        return self.proj(x)\n",
    "    \n",
    "flat_patch_embed = PatchEmbed(patch_dim, embed_dim)(flat_patches)  # shape: (60000, 16, 64)\n",
    "print(flat_patch_embed.shape)  # Should print: torch.Size([60000, 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01fd9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CLS token and concatenate it to the patch embeddings\n",
    "\n",
    "B, N, D = flat_patch_embed.shape\n",
    "\n",
    "# Create a learnable CLS token\n",
    "class_token = nn.Parameter(torch.zeros(1, 1, D))  # (1, 1, 64)\n",
    "nn.init.trunc_normal_(class_token, std=0.02)  # paper uses truncated normal for init\n",
    "\n",
    "# Expand CLS token for the batch\n",
    "cls_tokens = class_token.expand(B, -1, -1)  # (60000, 1, 64)\n",
    "\n",
    "# Concatenate to the front of the patch embeddings\n",
    "vit_input = torch.cat([cls_tokens, flat_patch_embed], dim=1)  # (60000, 17, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f601e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add positional encoding to the patch embeddings and CLS token\n",
    "\n",
    "# Create positional embeddings (learnable)\n",
    "pos_embed = nn.Parameter(torch.zeros(1, N + 1, D))  # (1, 17, 64)\n",
    "nn.init.trunc_normal_(pos_embed, std=0.02)\n",
    "\n",
    "# Add positional encoding to vit_input\n",
    "vit_input = vit_input + pos_embed  # (60000, 17, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbbf9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTEncoder(nn.Module):\n",
    "    \"\"\"Visual Transformer encoder for the MNIST dataset.\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
