{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9137816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14b6fae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. Load MSNIST dataset\n",
    "# -----------------------------\n",
    "\n",
    "# This will stream the data, you don't have to download the full file\n",
    "# mnist_train = load_dataset(\"ylecun/mnist\", split=\"train\")\n",
    "\n",
    "# mnist_test = load_dataset(\"ylecun/mnist\", split=\"test\")\n",
    "\n",
    "\n",
    "##### Look into the normalisation #####\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "mnist_train = datasets.MNIST(\"./data\", train=True, download=False, transform=transform)\n",
    "\n",
    "mnist_test = datasets.MNIST(\"./data\", train=False, download=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bc1349b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "img, label = mnist_train[0]\n",
    "print(img.shape)    # e.g., torch.Size([1, 28, 28])\n",
    "print(type(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a00c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch(img, patch_size=7):\n",
    "    # img shape: (1, 28, 28)\n",
    "    patches = img.unfold(1, patch_size, patch_size).unfold(2, patch_size, patch_size)\n",
    "    # shape: (1, 4, 4, 7, 7)\n",
    "    patches = patches.contiguous().view(1, -1, patch_size, patch_size)\n",
    "    # shape: (1, 16, 7, 7)\n",
    "    return patches.squeeze(0)  # (16, 7, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baa66336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "img, label = mnist_train[0]  # img: (1, 28, 28)\n",
    "patches = patch(img, patch_size=7)\n",
    "print(patches.shape)  # Should print: torch.Size([16, 7, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5844bd4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, _ = mnist_train[0]\n",
    "patches = patch(img)\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(6, 6))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(patches[i], cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "703d37d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_patches = [patch(img) for img, _ in mnist_train]  # list of (16,7,7)\n",
    "all_patches = torch.stack(all_patches)  # (N, 16, 7, 7)\n",
    "all_labels = torch.tensor([label for _, label in mnist_train])  # shape: (60000,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7450918c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 16, 7, 7])\n",
      "torch.Size([60000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/site-packages/IPython/core/completer.py\", line 3496, in _complete\n",
      "    result = matcher(context)\n",
      "             ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/IPython/core/completer.py\", line 2253, in magic_matcher\n",
      "    global_matches = self.global_matches(bare_text)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/IPython/core/completer.py\", line 1132, in global_matches\n",
      "    for word in lst:\n",
      "                ^^^\n",
      "RuntimeError: dictionary changed size during iteration\n"
     ]
    }
   ],
   "source": [
    "print(all_patches.shape)  # Should print: torch.Size([60000, 16, 7, 7])\n",
    "print(all_labels.shape)  # Should print: torch.Size([60000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b161c899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 16, 49])\n"
     ]
    }
   ],
   "source": [
    "flat_patches = all_patches.view(60000, 16, -1)  # shape: (60000, 16, 49)\n",
    "print(flat_patches.shape)  # Should print: torch.Size([60000, 16, 49])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "663cb22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "patch_size = 7\n",
    "embed_dim = 64\n",
    "patch_dim = patch_size * patch_size\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"A simple linear projection of patches to embeddings.\"\"\"\n",
    "    def __init__(self, patch_dim, embed_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(patch_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, num_patches, patch_dim)\n",
    "        return self.proj(x)\n",
    "    \n",
    "flat_patch_embed = PatchEmbed(patch_dim, embed_dim)(flat_patches)  # shape: (60000, 16, 64)\n",
    "print(flat_patch_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a01fd9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CLS token and concatenate it to the patch embeddings\n",
    "\n",
    "B, N, D = flat_patch_embed.shape\n",
    "\n",
    "# Create a learnable CLS token\n",
    "class_token = nn.Parameter(torch.zeros(1, 1, D))  # (1, 1, 64)\n",
    "nn.init.trunc_normal_(class_token, std=0.02)  # paper uses truncated normal for init\n",
    "\n",
    "# Expand CLS token for the batch\n",
    "cls_tokens = class_token.expand(B, -1, -1)  # (60000, 1, 64)\n",
    "\n",
    "# Concatenate to the front of the patch embeddings\n",
    "vit_input = torch.cat([cls_tokens, flat_patch_embed], dim=1)  # (60000, 17, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6394e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[-0.0228,  0.0049, -0.0077, -0.0052,  0.0391, -0.0071,  0.0343,\n",
      "          -0.0206, -0.0264, -0.0020,  0.0072,  0.0371,  0.0134, -0.0102,\n",
      "           0.0050,  0.0039, -0.0222,  0.0034,  0.0100, -0.0017, -0.0213,\n",
      "          -0.0030, -0.0033, -0.0162, -0.0116,  0.0127, -0.0206,  0.0041,\n",
      "           0.0116,  0.0284, -0.0166, -0.0261, -0.0070, -0.0237,  0.0406,\n",
      "           0.0195, -0.0050, -0.0240, -0.0132,  0.0047,  0.0005, -0.0016,\n",
      "          -0.0148, -0.0376, -0.0013, -0.0298,  0.0248,  0.0185,  0.0103,\n",
      "          -0.0019, -0.0246, -0.0227, -0.0153,  0.0065, -0.0344,  0.0397,\n",
      "           0.0281, -0.0251,  0.0088,  0.0298, -0.0276,  0.0139, -0.0108,\n",
      "          -0.0135]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[ 1.9910e-02, -1.4961e-02, -3.3216e-02,  2.9911e-03, -2.6402e-02,\n",
      "          -2.2968e-02, -1.7589e-02, -1.1884e-02,  2.3161e-02,  3.5351e-03,\n",
      "          -8.5228e-03, -1.0269e-02, -3.0990e-02,  4.8729e-03,  6.8288e-05,\n",
      "           1.2698e-02,  5.1579e-03, -2.7870e-02, -3.9754e-02,  3.3548e-03,\n",
      "           1.4324e-02, -2.2745e-02,  2.1502e-02,  1.6325e-02, -7.3639e-03,\n",
      "          -2.2390e-02,  6.3909e-03,  1.7835e-02, -1.7314e-03,  5.2625e-03,\n",
      "           2.2200e-02,  2.2705e-02, -2.0772e-02, -2.3014e-02,  1.2724e-02,\n",
      "          -2.4722e-03, -1.4527e-03, -1.0812e-03,  3.3173e-02,  2.1604e-02,\n",
      "          -2.6160e-03, -3.3968e-02, -1.1793e-02,  3.3339e-02, -8.4011e-04,\n",
      "          -4.4086e-04, -5.8794e-03,  2.3450e-02,  1.2958e-02,  1.8487e-03,\n",
      "          -4.2026e-03,  4.7656e-02,  1.9955e-04, -5.3013e-03,  3.4082e-02,\n",
      "          -3.4980e-03,  3.7092e-02,  1.3367e-02,  3.2941e-03,  4.5571e-03,\n",
      "          -1.8673e-02,  9.8994e-03,  9.9297e-03, -8.2869e-03]]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(class_token)\n",
    "print(nn.init.trunc_normal_(class_token, std=0.02))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8f601e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add positional encoding to the patch embeddings and CLS token\n",
    "\n",
    "# Create positional embeddings (learnable)\n",
    "pos_embed = nn.Parameter(torch.zeros(1, N + 1, D))  # (1, 17, 64)\n",
    "nn.init.trunc_normal_(pos_embed, std=0.02)\n",
    "\n",
    "# Add positional encoding to vit_input\n",
    "vit_input = vit_input + pos_embed  # (60000, 17, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2ed0b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[ 4.7437e-03, -6.4648e-03,  2.4761e-02,  ..., -7.9734e-02,\n",
      "          -2.5453e-03,  4.5758e-02],\n",
      "         [ 7.6552e-04, -2.0545e-02, -3.2624e-03,  ...,  6.9082e-03,\n",
      "           2.0913e-02,  7.6138e-06],\n",
      "         [ 2.5296e-02, -1.9518e-05,  2.7421e-02,  ..., -6.0568e-03,\n",
      "           5.5372e-02, -4.3617e-02],\n",
      "         ...,\n",
      "         [-3.6805e-03,  3.0003e-02,  8.8548e-03,  ...,  3.2514e-02,\n",
      "           3.0161e-03, -6.0853e-03],\n",
      "         [ 3.0060e-02,  2.4509e-02,  7.3096e-03,  ..., -2.4491e-02,\n",
      "          -2.4062e-02, -4.6620e-03],\n",
      "         [ 9.1824e-03, -8.2403e-03, -6.9390e-03,  ...,  2.1619e-02,\n",
      "          -1.4267e-02,  4.9546e-03]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[-0.0031, -0.0267,  0.0136,  ..., -0.0383, -0.0104, -0.0110],\n",
      "         [ 0.0115,  0.0256,  0.0232,  ...,  0.0074, -0.0241,  0.0085],\n",
      "         [-0.0166,  0.0074,  0.0122,  ...,  0.0213,  0.0296, -0.0091],\n",
      "         ...,\n",
      "         [-0.0058, -0.0204,  0.0176,  ..., -0.0161,  0.0300,  0.0124],\n",
      "         [-0.0416, -0.0250,  0.0004,  ..., -0.0258, -0.0046,  0.0133],\n",
      "         [-0.0153, -0.0199, -0.0183,  ..., -0.0410, -0.0161,  0.0294]]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(pos_embed)\n",
    "print(nn.init.trunc_normal_(pos_embed, std=0.02))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bbbf9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Visual Transformer encoder for the MNIST dataset.\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the encoder block.\"\"\"\n",
    "        # x shape: (B, N, D) where B is batch size, N is number of patches + 1 (CLS token), D is embedding dimension\n",
    "        x_res1 = x\n",
    "        x = self.ln1(x)\n",
    "        x, _ = self.attn(x, x, x)  # Self-attention\n",
    "        x = x + x_res1             # Residual connection 1\n",
    "\n",
    "        x_res2 = x\n",
    "        x = self.ln2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = x + x_res2             # Residual connection 2\n",
    "        return x\n",
    "    \n",
    "class VisualTransformer(nn.Module):\n",
    "    \"\"\"Visual Transformer for the MNIST dataset.\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, num_layers, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderBlock(embed_dim, num_heads) for _ in range(num_layers)])\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the Visual Transformer.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.ln(x)\n",
    "        return self.head(x[:, 0, :])  # Use CLS token for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2149d0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 10])\n",
      "tensor([-0.4947, -0.0908, -0.1558, -0.5772, -0.9769,  0.7581,  0.1659, -0.1473,\n",
      "        -0.8144, -0.2715], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example with first 128 images:\n",
    "B = 128\n",
    "test_x = vit_input[:B]           # shape: (128, 17, 64)\n",
    "test_labels = all_labels[:B]     # shape: (128,)\n",
    "\n",
    "num_heads = 4\n",
    "num_layers = 3  \n",
    "\n",
    "model = VisualTransformer(embed_dim=embed_dim, num_heads=num_heads, num_layers=num_layers)\n",
    "\n",
    "out = model(test_x)              # Should be (128, 10)\n",
    "print(out.shape)\n",
    "print(out[0])  # Print the output for the first image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9b0656",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1875/1875 [01:06<00:00, 27.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.4274, Accuracy: 80.9100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1875/1875 [01:34<00:00, 19.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.2263, Accuracy: 90.9433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1875/1875 [02:52<00:00, 10.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.2976, Accuracy: 93.2117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|█████▊    | 1097/1875 [03:35<01:17, 10.03it/s]"
     ]
    }
   ],
   "source": [
    "# Training loop with cross entropy loss and Adam optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "embed_dim = 64\n",
    "num_heads = 4\n",
    "num_layers = 3\n",
    "\n",
    "model = VisualTransformer(embed_dim=embed_dim, num_heads=num_heads, num_layers=num_layers)\n",
    "model = model.to(device)\n",
    "\n",
    "# Loop through parameters and set requires_grad to True\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "dataset = TensorDataset(vit_input, all_labels)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    correct_total, sample_total = 0, 0\n",
    "    model.train()\n",
    "    for x_batch, y_batch in tqdm(dataloader, desc=\"Training\", total=len(dataloader)):\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        logits = model(x_batch)\n",
    "        loss = loss_fn(logits, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = logits.argmax(dim=1)  # (B,)\n",
    "        correct_preds = (preds == y_batch).sum().item()\n",
    "        sample_total += len(y_batch)\n",
    "        correct_total += correct_preds\n",
    "\n",
    "    epoch_accuracy = (correct_total / sample_total) *100\n",
    "    print(f\"Epoch {i+1}/{num_epochs}, Loss: {loss.item():.4f}, Accuracy: {epoch_accuracy:.2f}\")\n",
    "\n",
    "torch.save(staticmethod(model.state_dict()), \"mnist_vit_encoder.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7464753c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
