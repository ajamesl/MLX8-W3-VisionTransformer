{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad080a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c374a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load test set & model ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "mnist_test = datasets.MNIST(root=\"./data\", train=False, download=False, transform=transform)\n",
    "\n",
    "def stitch_and_resize(images, labels, out_size=img_size, min_scale=0.8, max_scale=1.7):\n",
    "    images = images.squeeze(1)\n",
    "    labels = torch.tensor(labels)\n",
    "    N = len(images)\n",
    "\n",
    "    canvas = torch.zeros((out_size, out_size), dtype=images.dtype, device=images.device)\n",
    "    occupied_mask = torch.zeros((out_size, out_size), dtype=torch.bool, device=images.device)\n",
    "    centers = []\n",
    "\n",
    "    max_attempts = 500\n",
    "    for i in range(N):\n",
    "        scale = random.uniform(min_scale, max_scale)\n",
    "        orig_digit_size = images[i].shape[-1]\n",
    "        new_digit_size = int(orig_digit_size * scale)\n",
    "        new_digit_size = max(8, min(new_digit_size, out_size))  # avoid too small or too big\n",
    "        digit_resized = TF.resize(images[i].unsqueeze(0), [new_digit_size, new_digit_size]).squeeze(0)\n",
    "        \n",
    "        placed = False\n",
    "        for _ in range(max_attempts):\n",
    "            x = random.randint(0, out_size - new_digit_size)\n",
    "            y = random.randint(0, out_size - new_digit_size)\n",
    "            region = occupied_mask[y:y+new_digit_size, x:x+new_digit_size]\n",
    "            if not region.any():\n",
    "                canvas[y:y+new_digit_size, x:x+new_digit_size] = digit_resized\n",
    "                occupied_mask[y:y+new_digit_size, x:x+new_digit_size] = True\n",
    "                center_x = x + new_digit_size // 2\n",
    "                center_y = y + new_digit_size // 2\n",
    "                centers.append((center_x, center_y, labels[i].item()))\n",
    "                placed = True\n",
    "                break\n",
    "        if not placed:\n",
    "            raise RuntimeError(f\"Could not place digit {i} after {max_attempts} attempts. Try fewer digits.\")\n",
    "\n",
    "    # Improved row grouping (same as before)\n",
    "    centers = sorted(centers, key=lambda t: t[1])  # sort by y\n",
    "    rows = []\n",
    "    row = [centers[0]]\n",
    "    row_height_thresh = int(28 * 0.8)  # you can set this relative to orig digit size or avg new size\n",
    "    for c in centers[1:]:\n",
    "        avg_y = sum([d[1] for d in row]) / len(row)\n",
    "        if abs(c[1] - avg_y) <= row_height_thresh:\n",
    "            row.append(c)\n",
    "        else:\n",
    "            rows.append(row)\n",
    "            row = [c]\n",
    "    if row:\n",
    "        rows.append(row)\n",
    "\n",
    "    rows = sorted(rows, key=lambda r: sum([d[1] for d in r]) / len(r))\n",
    "    centers_ordered = []\n",
    "    for row in rows:\n",
    "        row_sorted = sorted(row, key=lambda t: t[0])\n",
    "        centers_ordered.extend(row_sorted)\n",
    "    sorted_labels = torch.tensor([label for _, _, label in centers_ordered], dtype=labels.dtype, device=labels.device)\n",
    "    stitched = canvas.unsqueeze(0)\n",
    "    return stitched, sorted_labels\n",
    "\n",
    "\n",
    "# --- Custom Dataset ---\n",
    "class CustomMNISTDataset(torch.utils.data.Dataset):\n",
    "    \"\"\" Custom Dataset for MNIST that stitches a given number of images together \"\"\"\n",
    "    def __init__(self, mnist_dataset, length=60000):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "        self.length = length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get N random images and their labels\n",
    "        num = torch.randint(1, 11, (1,)).item()  # Randomly choose between 1 and 10\n",
    "        idxs = torch.randint(0, len(self.mnist_dataset), (num,)) # Random indices\n",
    "        # Get images and labels from the dataset\n",
    "        imgs, labels = zip(*(self.mnist_dataset[i.item()] for i in idxs))\n",
    "        images = torch.stack(imgs)\n",
    "        stitched_image, stitched_label = stitch_and_resize(images, labels)\n",
    "        stitched_label = torch.cat([stitched_label, torch.tensor([11], device=stitched_label.device, dtype=stitched_label.dtype)])\n",
    "        return stitched_image, stitched_label\n",
    "\n",
    "# --- Patch Embedding ---\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Patch Embedding Layer for Vision Transformer\n",
    "    Args:\"\"\"\n",
    "    def __init__(self, patch_size=patch_size, embed_dim=embed_dim, img_size=img_size, in_chans=1):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_patches = num_patches\n",
    "        self.proj = nn.Linear(patch_size * patch_size * in_chans, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 1, 256, 256)\n",
    "        B, C, H, W = x.shape\n",
    "        patches = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "        # patches: (B, C, num_patches_h, num_patches_w, patch_size, patch_size)\n",
    "        patches = patches.contiguous().view(B, C, -1, self.patch_size, self.patch_size)\n",
    "        # patches: (B, C, num_patches, patch_size, patch_size)\n",
    "        patches = patches.permute(0, 2, 1, 3, 4)  # (B, num_patches, C, 16, 16)\n",
    "        patches = patches.reshape(B, self.num_patches, -1)  # (B, num_patches, patch_size*patch_size*C) / (B, 256, 256)\n",
    "        return self.proj(patches)  # (B, 256, embed_dim)\n",
    "\n",
    "# --- Attention Mechanism ---\n",
    "class Head(nn.Module):\n",
    "    \"\"\"One head of attention.\"\"\"\n",
    "    def __init__(self, head_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_dim, head_size, bias=False)\n",
    "\n",
    "    def forward(self, x, y, mask=None):\n",
    "        B, T , _ = x.shape\n",
    "        q = self.query(x)  # Shape: (B, T, head_size)\n",
    "        k = self.key(y)  # Shape: (B, T, head_size)\n",
    "        v = self.value(y)  # Shape: (B, T, head_size)\n",
    "        # Compute attention scores (\"affinities\")\n",
    "        attn = (q @ k.transpose(-2, -1)) * (self.head_size**-0.5)  # (B, T, head_size) @ (B, head_size, T) ---> (B, T, T)\n",
    "        if mask is not None:           \n",
    "            attn = attn.masked_fill(mask == 0, float('-inf')) # (B, T, T)\n",
    "        attn = F.softmax(attn, dim=-1) # (B, T, T)\n",
    "        out = attn @ v  # (B, T, T) @ (B, T, head_size) ---> (B, T, head_size)\n",
    "        return out\n",
    "\n",
    "# --- Multi-Head Attention ---  \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-heads of attention in parallel.\"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = embed_dim // num_heads\n",
    "        assert self.head_size * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "        self.heads = nn.ModuleList([Head(head_size, embed_dim) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x, y, mask=None):\n",
    "        out = torch.cat([h(x, y, mask) for h in self.heads], dim=-1) # Concatenate the outputs of all heads (B, T, num_heads * head_size) = (B, T, embed_dim)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "# --- Encoder Block ---\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadAttention(num_heads, embed_dim // num_heads)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_res1 = x\n",
    "        x = self.ln1(x)\n",
    "        x = self.attn(x, x)\n",
    "        x = x + x_res1\n",
    "\n",
    "        x_res2 = x\n",
    "        x = self.ln2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = x + x_res2\n",
    "        return x\n",
    "\n",
    "# --- Decoder Block ---\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.masked_attn = MultiHeadAttention(num_heads, embed_dim // num_heads)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.cross_attn = MultiHeadAttention(num_heads, embed_dim // num_heads)\n",
    "        self.ln3 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, enc_out, mask):\n",
    "        x_res1 = x\n",
    "        x = self.ln1(x)\n",
    "        x = self.masked_attn(x, x, mask=mask)\n",
    "        x = x + x_res1\n",
    "\n",
    "        x_res2 = x\n",
    "        x = self.cross_attn(x, enc_out)\n",
    "\n",
    "        x = self.ln2(x)\n",
    "        x = x + x_res2\n",
    "\n",
    "        x_res3 = x\n",
    "        x = self.mlp(x)\n",
    "        x = x + x_res3\n",
    "        return x\n",
    "    \n",
    "# --- Visual Transformer ---\n",
    "class VisualTransformer(nn.Module):\n",
    "    def __init__(self, patch_size, embed_dim, num_heads, num_layers, num_classes, img_size=img_size, in_chans=1, seq_len=11):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(patch_size, embed_dim, img_size, in_chans)\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        self.pos_encod_enc = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        nn.init.trunc_normal_(self.pos_encod_enc, std=0.02)\n",
    "\n",
    "        self.encoder = nn.ModuleList([EncoderBlock(embed_dim, num_heads) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_size = num_classes + 3   # +1 for start token\n",
    "        self.tok_embed = nn.Embedding(self.vocab_size, embed_dim)\n",
    "        self.pos_encod_dec = nn.Parameter(torch.zeros(1, seq_len, embed_dim))\n",
    "        nn.init.trunc_normal_(self.pos_encod_dec, std=0.02)\n",
    "\n",
    "        self.decoder = nn.ModuleList([DecoderBlock(embed_dim, num_heads) for _ in range(num_layers)])\n",
    "        self.linear = nn.Linear(embed_dim, self.vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # x: (B, 1, 256, 256)\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)                        # (B, 256, embed_dim)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, embed_dim)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)          # (B, 257, embed_dim)\n",
    "        x = x + self.pos_encod_enc                     # (B, 257, embed_dim)\n",
    "        for block in self.encoder:\n",
    "            x = block(x)\n",
    "        x = self.norm(x)                               # (B, 257, embed_dim)\n",
    "\n",
    "        # y: (B, seq_len)\n",
    "        y = self.tok_embed(y)                                 # (B, seq_len, embed_dim)\n",
    "        curr_seq_len = y.shape[1]\n",
    "        pos_encod_dec = self.pos_encod_dec[:, :curr_seq_len, :].expand(B, curr_seq_len, -1)\n",
    "        y = y + pos_encod_dec                           # (B, seq_len, embed_dim)\n",
    "        mask = torch.triu(torch.ones((curr_seq_len, curr_seq_len), device=x.device), diagonal=1).bool()\n",
    "        for block in self.decoder:\n",
    "            y = block(y, x, mask=mask)      # (B, seq_len, embed_dim)\n",
    "        out = self.linear(y)                # (B, seq_len, vocab_size)\n",
    "        return out\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    correct_tokens, total_tokens = 0, 0\n",
    "    correct_seqs, total_seqs = 0, 0\n",
    "    pad_token = 12\n",
    "    eos_token = 11\n",
    "    start_token = 10\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y, y_lens in data_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            B, seq_len = y.shape\n",
    "\n",
    "            # Greedy decoding with start token\n",
    "            y_input = torch.full((B, 1), start_token, dtype=torch.long, device=device)\n",
    "            preds = []\n",
    "\n",
    "            for t in range(seq_len):\n",
    "                out = model(x, y_input)  # (B, t+1, vocab_size)\n",
    "                next_token = out[:, -1, :].argmax(dim=-1, keepdim=True)  # (B, 1)\n",
    "                preds.append(next_token)\n",
    "                y_input = torch.cat([y_input, next_token], dim=1)\n",
    "                # Optional: break if all seqs have EOS (can add optimization)\n",
    "\n",
    "            preds = torch.cat(preds, dim=1)  # (B, seq_len)\n",
    "\n",
    "            # --- Per-token accuracy: ignore PAD in target ---\n",
    "            mask = (y != pad_token)\n",
    "            correct_tokens += (preds[mask] == y[mask]).sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "\n",
    "            # --- Per-sequence accuracy (all tokens up to EOS or PAD must match) ---\n",
    "            for i in range(B):\n",
    "                # Get reference (ground truth) up to EOS or PAD\n",
    "                gt_seq = []\n",
    "                for tok in y[i].tolist():\n",
    "                    if tok == eos_token or tok == pad_token:\n",
    "                        break\n",
    "                    gt_seq.append(tok)\n",
    "                # Get prediction up to EOS or PAD\n",
    "                pred_seq = []\n",
    "                for tok in preds[i].tolist():\n",
    "                    if tok == eos_token or tok == pad_token:\n",
    "                        break\n",
    "                    pred_seq.append(tok)\n",
    "                # Compare full sequence\n",
    "                if pred_seq == gt_seq:\n",
    "                    correct_seqs += 1\n",
    "                total_seqs += 1\n",
    "\n",
    "    token_acc = 100 * correct_tokens / total_tokens if total_tokens else 0\n",
    "    seq_acc = 100 * correct_seqs / total_seqs if total_seqs else 0\n",
    "\n",
    "    print(f\"Token Accuracy: {token_acc:.2f}% | Sequence Accuracy: {seq_acc:.2f}%\")\n",
    "    return token_acc, seq_acc\n",
    "\n",
    "def collate_fn(batch):\n",
    "    x_seqs, y_seqs = zip(*batch)\n",
    "    y_lens = [y.shape[0] for y in y_seqs]\n",
    "    x_batch = torch.stack(x_seqs)\n",
    "    y_padded = pad_sequence(y_seqs, batch_first=True, padding_value=12)\n",
    "    return x_batch, y_padded, y_lens\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer(model, img_tensor, seq_len=11, device='cpu'):\n",
    "    img_tensor = img_tensor.unsqueeze(0).to(device)  # shape (1, 1, 256, 256)\n",
    "    start_token = 10\n",
    "    y_input = torch.full((1, 1), start_token, dtype=torch.long, device=device)\n",
    "    preds = []\n",
    "    for _ in range(seq_len):\n",
    "        logits = model(img_tensor, y_input)\n",
    "        next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "        next_digit = next_token.item()\n",
    "        if next_digit == 11:\n",
    "            break\n",
    "        preds.append(next_digit)\n",
    "        y_input = torch.cat([y_input, next_token], dim=1)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310bb16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load model ---\n",
    "model = VisualTransformer(\n",
    "    patch_size=16,\n",
    "    embed_dim=64,\n",
    "    num_heads=4,\n",
    "    num_layers=4,\n",
    "    num_classes=10,\n",
    "    img_size=256,\n",
    "    seq_len=11,\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load('mnist_vit_multi_final_attn.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Print total and trainable parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters:     {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "test_dataset_stitch = CustomMNISTDataset(mnist_test, length=5000)\n",
    "test_loader_stitch = DataLoader(test_dataset_stitch, batch_size=64, shuffle=False, collate_fn=collate_fn, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0415da7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Qualitative test (repeat as desired) ---\n",
    "num_digits = random.randint(2, 8)   # You can set this to any number in [1,10]\n",
    "sample_idxs = random.sample(range(len(mnist_test)), num_digits)\n",
    "images = torch.stack([mnist_test[i][0] for i in sample_idxs])\n",
    "labels = [mnist_test[i][1] for i in sample_idxs]\n",
    "\n",
    "stitched_img, true_labels = stitch_and_resize(images, labels, out_size=256)\n",
    "preds = infer(model, stitched_img, seq_len=11, device=device)\n",
    "\n",
    "print(\"True sequence:\", true_labels.cpu().numpy())\n",
    "print(\"Predicted sequence:\", preds)\n",
    "\n",
    "plt.imshow(stitched_img.squeeze().cpu().numpy(), cmap=\"gray\")\n",
    "plt.title(f\"GT: {''.join(str(x) for x in true_labels.cpu().numpy())} | PR: {''.join(str(x) for x in preds)}\")\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38200e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, test_loader_stitch)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
