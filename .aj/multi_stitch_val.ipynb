{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8061f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f71a864f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hyperparameters (as used in training) ---\n",
    "batch_size = 128\n",
    "patch_size = 16\n",
    "embed_dim = 64\n",
    "num_heads = 4\n",
    "num_layers = 3\n",
    "num_classes = 10\n",
    "img_size = 256\n",
    "seq_len = 11\n",
    "data_path = \"./data\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def stitch_and_resize(images, labels, out_size=img_size):\n",
    "    \"\"\"\n",
    "    images: Tensor of shape (N, 1, 28, 28)\n",
    "    labels: Tensor of N integers representing the labels of the images\n",
    "    Returns: Tensor of shape (1, out_size, out_size) and a tensor of labels of shape (N,)\n",
    "    \"\"\"\n",
    "    # Squeeze channel for concatenation\n",
    "    images = images.squeeze(1)  # (N, 28, 28)\n",
    "    # Extract the label from each image and append in order of image selection\n",
    "    labels = torch.tensor(labels)  # (N,)\n",
    "    N = len(images)\n",
    "    grid_size = math.ceil(math.sqrt(N))\n",
    "    pad_needed = grid_size**2 - N\n",
    "    if pad_needed > 0:\n",
    "        blank = torch.zeros((28, 28), dtype=images.dtype, device=images.device)\n",
    "        # Add pad_needed blank images to fill the grid\n",
    "        images = torch.cat([images, blank.unsqueeze(0).repeat(pad_needed, 1, 1)], dim=0)\n",
    "\n",
    "    rows = []\n",
    "    for r in range(grid_size):\n",
    "        row_imgs = images[r*grid_size:(r+1)*grid_size]  # shape: (cols, 28, 28)\n",
    "        row_cat = torch.cat(list(row_imgs), dim=1)      # concat horizontally\n",
    "        rows.append(row_cat)\n",
    "    \n",
    "    # Concatenate all rows vertically\n",
    "    stitched = torch.cat(rows, dim=0).unsqueeze(0)   # vertical, shape: (1, H, W)\n",
    "\n",
    "    # Now resize to (1, out_size, out_size)\n",
    "    stitched_resized = TF.resize(stitched, [out_size, out_size])\n",
    "    return stitched_resized, labels\n",
    "\n",
    "# --- Custom Dataset ---\n",
    "class CustomMNISTDataset(torch.utils.data.Dataset):\n",
    "    \"\"\" Custom Dataset for MNIST that stitches a given number of images together \"\"\"\n",
    "    def __init__(self, mnist_dataset, length=60000):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "        self.length = length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get N random images and their labels\n",
    "        num = torch.randint(1, 11, (1,)).item()  # Randomly choose between 1 and 10\n",
    "        idxs = torch.randint(0, len(self.mnist_dataset), (num,)) # Random indices\n",
    "        # Get images and labels from the dataset\n",
    "        imgs, labels = zip(*(self.mnist_dataset[i.item()] for i in idxs))\n",
    "        labels = list(labels) + [11]\n",
    "        images = torch.stack(imgs)\n",
    "        stitched_image, stitched_label = stitch_and_resize(images, labels)\n",
    "        return stitched_image, stitched_label\n",
    "\n",
    "# --- Patch Embedding ---\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Patch Embedding Layer for Vision Transformer\n",
    "    Args:\"\"\"\n",
    "    def __init__(self, patch_size=patch_size, embed_dim=embed_dim, img_size=img_size, in_chans=1):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_patches = num_patches\n",
    "        self.proj = nn.Linear(patch_size * patch_size * in_chans, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 1, 256, 256)\n",
    "        B, C, H, W = x.shape\n",
    "        patches = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "        # patches: (B, C, num_patches_h, num_patches_w, patch_size, patch_size)\n",
    "        patches = patches.contiguous().view(B, C, -1, self.patch_size, self.patch_size)\n",
    "        # patches: (B, C, num_patches, patch_size, patch_size)\n",
    "        patches = patches.permute(0, 2, 1, 3, 4)  # (B, num_patches, C, 16, 16)\n",
    "        patches = patches.reshape(B, self.num_patches, -1)  # (B, num_patches, patch_size*patch_size*C) / (B, 256, 256)\n",
    "        return self.proj(patches)  # (B, 256, embed_dim)\n",
    "    \n",
    "# --- Encoder Block ---\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_res1 = x\n",
    "        x = self.ln1(x)\n",
    "        x, _ = self.attn(x, x, x)\n",
    "        x = x + x_res1\n",
    "\n",
    "        x_res2 = x\n",
    "        x = self.ln2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = x + x_res2\n",
    "        return x\n",
    "\n",
    "# --- Decoder Block ---\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.masked_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.ln3 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, enc_out, mask):\n",
    "        x_res1 = x\n",
    "        x = self.ln1(x)\n",
    "        x, _ = self.masked_attn(x, x, x, attn_mask=mask)\n",
    "        x = x + x_res1\n",
    "\n",
    "        x_res2 = x\n",
    "        x, _ = self.cross_attn(x, enc_out, enc_out)\n",
    "\n",
    "        x = self.ln2(x)\n",
    "        x = x + x_res2\n",
    "\n",
    "        x_res3 = x\n",
    "        x = self.mlp(x)\n",
    "        x = x + x_res3\n",
    "        return x\n",
    "    \n",
    "# --- Visual Transformer ---\n",
    "class VisualTransformer(nn.Module):\n",
    "    def __init__(self, patch_size, embed_dim, num_heads, num_layers, num_classes, img_size=img_size, in_chans=1, seq_len=11):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(patch_size, embed_dim, img_size, in_chans)\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        self.pos_encod_enc = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        nn.init.trunc_normal_(self.pos_encod_enc, std=0.02)\n",
    "\n",
    "        self.encoder = nn.ModuleList([EncoderBlock(embed_dim, num_heads) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_size = num_classes + 3   # +1 for start token\n",
    "        self.tok_embed = nn.Embedding(self.vocab_size, embed_dim)\n",
    "        self.pos_encod_dec = nn.Parameter(torch.zeros(1, seq_len, embed_dim))\n",
    "        nn.init.trunc_normal_(self.pos_encod_dec, std=0.02)\n",
    "\n",
    "        self.decoder = nn.ModuleList([DecoderBlock(embed_dim, num_heads) for _ in range(num_layers)])\n",
    "        self.linear = nn.Linear(embed_dim, self.vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # x: (B, 1, 256, 256)\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)                        # (B, 256, embed_dim)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, embed_dim)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)          # (B, 257, embed_dim)\n",
    "        x = x + self.pos_encod_enc                     # (B, 257, embed_dim)\n",
    "        for block in self.encoder:\n",
    "            x = block(x)\n",
    "        x = self.norm(x)                               # (B, 257, embed_dim)\n",
    "\n",
    "        # y: (B, seq_len)\n",
    "        y = self.tok_embed(y)                                 # (B, seq_len, embed_dim)\n",
    "        curr_seq_len = y.shape[1]\n",
    "        pos_encod_dec = self.pos_encod_dec[:, :curr_seq_len, :].expand(B, curr_seq_len, -1)\n",
    "        y = y + pos_encod_dec                           # (B, seq_len, embed_dim)\n",
    "        mask = torch.triu(torch.ones((curr_seq_len, curr_seq_len), device=x.device), diagonal=1).bool()\n",
    "        for block in self.decoder:\n",
    "            y = block(y, x, mask=mask)      # (B, seq_len, embed_dim)\n",
    "        out = self.linear(y)                # (B, seq_len, vocab_size)\n",
    "        return out\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    x_seqs, y_seqs = zip(*batch)\n",
    "    y_lens = [y.shape[0] for y in y_seqs]\n",
    "    x_batch = torch.stack(x_seqs)\n",
    "    y_padded = pad_sequence(y_seqs, batch_first=True, padding_value=12)\n",
    "    return x_batch, y_padded, y_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca275f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare Test Data ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "mnist_test = datasets.MNIST(root=data_path, train=False, download=False, transform=transform)\n",
    "test_dataset_stitch = CustomMNISTDataset(mnist_test, length=5000)  # Use a subset for faster eval if you wish\n",
    "test_loader_stitch = DataLoader(test_dataset_stitch, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543661ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Model ---\n",
    "model = VisualTransformer(\n",
    "    patch_size=patch_size,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    num_classes=num_classes,\n",
    "    img_size=img_size,\n",
    "    seq_len=seq_len\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load('mnist_vit_multi_stitch.pth', map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1a883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot and Predict on First Few Test Samples ---\n",
    "pad_token = 12\n",
    "eos_token = 11\n",
    "start_token = 10\n",
    "\n",
    "for i in range(4):\n",
    "    stitched_img, label_tensor = test_dataset_stitch[i]\n",
    "    plt.figure(figsize=(3,3))\n",
    "    plt.imshow(stitched_img.squeeze().cpu().numpy(), cmap=\"gray\")\n",
    "    # Display ground truth (exclude EOS, PAD)\n",
    "    gt_digits = [d.item() for d in label_tensor if d.item() not in [eos_token, pad_token]]\n",
    "    plt.title(f\"GT: {''.join(str(d) for d in gt_digits)}\")\n",
    "\n",
    "    # --- Run model inference (greedy decoding) ---\n",
    "    with torch.no_grad():\n",
    "        x = stitched_img.unsqueeze(0).to(device)\n",
    "        y_input = torch.full((1, 1), start_token, dtype=torch.long, device=device)\n",
    "        preds = []\n",
    "        for t in range(seq_len):\n",
    "            out = model(x, y_input)\n",
    "            next_token = out[0, -1].argmax().item()\n",
    "            preds.append(next_token)\n",
    "            if next_token == eos_token:\n",
    "                break\n",
    "            y_input = torch.cat([y_input, torch.tensor([[next_token]], device=device)], dim=1)\n",
    "    pred_digits = [d for d in preds if d not in [start_token, eos_token, pad_token]]\n",
    "    print(f\"Ground Truth: {''.join(str(d) for d in gt_digits)}\")\n",
    "    print(f\"Prediction :  {''.join(str(d) for d in pred_digits)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ed1c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Full Test Set Evaluation (Token & Seq Accuracy) ---\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    correct_tokens, total_tokens = 0, 0\n",
    "    correct_seqs, total_seqs = 0, 0\n",
    "    pad_token = 12\n",
    "    eos_token = 11\n",
    "    start_token = 10\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y, y_lens in data_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            B, seq_len = y.shape\n",
    "            # Greedy decode with start token\n",
    "            y_input = torch.full((B, 1), start_token, dtype=torch.long, device=device)\n",
    "            preds = []\n",
    "            for t in range(seq_len):\n",
    "                out = model(x, y_input)\n",
    "                next_token = out[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "                preds.append(next_token)\n",
    "                y_input = torch.cat([y_input, next_token], dim=1)\n",
    "            preds = torch.cat(preds, dim=1)  # (B, seq_len)\n",
    "\n",
    "            # Per-token accuracy\n",
    "            mask = (y != pad_token)\n",
    "            correct_tokens += (preds[mask] == y[mask]).sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "            # Per-sequence accuracy (up to EOS or PAD)\n",
    "            for i in range(B):\n",
    "                gt_seq = []\n",
    "                for tok in y[i].tolist():\n",
    "                    if tok == eos_token or tok == pad_token:\n",
    "                        break\n",
    "                    gt_seq.append(tok)\n",
    "                pred_seq = []\n",
    "                for tok in preds[i].tolist():\n",
    "                    if tok == eos_token or tok == pad_token:\n",
    "                        break\n",
    "                    pred_seq.append(tok)\n",
    "                if pred_seq == gt_seq:\n",
    "                    correct_seqs += 1\n",
    "                total_seqs += 1\n",
    "    token_acc = 100 * correct_tokens / total_tokens if total_tokens else 0\n",
    "    seq_acc = 100 * correct_seqs / total_seqs if total_seqs else 0\n",
    "    print(f\"Token Accuracy: {token_acc:.2f}% | Sequence Accuracy: {seq_acc:.2f}%\")\n",
    "    return token_acc, seq_acc\n",
    "\n",
    "# Evaluate on test set\n",
    "evaluate(model, test_loader_stitch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
